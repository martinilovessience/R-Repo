testset <- dtm.train[-testindex, ]
index_data <- cbind(adtm.df, df.train)
corpus.ts <- corupus.train[testindex]
corpus.tss <- corupus.train[-testindex]
# Feature selection
dim(trainset)
# finding terms with lowest 2 and highest 6 frequency
ft <- findFreqTerms(trainset, 1, 8)
length((ft))
trainset <-
DocumentTermMatrix(corpus.ts, control = list(dictionary = ft))
testset <-
DocumentTermMatrix(corpus.tss, control = list(dictionary = ft))
#Train model with SVM
container <- create_container(trainset, df.train, trainSize = 1:nrow(trainset), virgin= FALSE)
pred.model <- train_model(container = container, algorithm = "SVM", kernel = "linear")
predicted <- predict(pred.model, newdata = testset)
table<- table("Predictions" = predicted, "Actual" = df.test)
print(table)
#Precision
nB_precision_ng<- round(table[1,1]/sum(table[,1]), 2)
nB_precision_nt<- round(table[2,2]/sum(table[,2]), 2)
nB_precision_p<- round(table[3,3]/sum(table[,3]), 2)
#Recall
nB_recall_ng<- round(table[1,1]/sum(table[1,]), 2)
nB_recall_nt<- round(table[2,2]/sum(table[2,]), 2)
nB_recall_p<- round(table[3,3]/sum(table[3,]), 2)
#Print Precision and Recall- Measures
print(c("Precision class 'Negative':", nB_precision_ng))
print(c("Recall class 'Negative':", nB_recall_ng))
print(c("Precision class 'Positive':",nB_precision_p))
print(c("Recall class 'Positive':",nB_recall_p))
print(c("Precision class 'Neutral':",nB_precision_nt))
print(c("Recall class 'Neutral':",nB_recall_nt))
#Print overall accuracy
print(paste("Accuracy:", (nB_precision_ng + nB_precision_nt + nB_precision_p + nB_recall_ng+ nB_recall_nt + nB_recall_p)/6))
## Result: The overall accuracy shows a maximum value to almost 0,6 which is insufficient.
# The precision- and recall values for the class 'Negative' are remarkeable low which is caused by the
# small percentage of negative comments in comparison with neutral and positve ones.
# In order to optimize the model the balance between all three classes should be more even as well as the amount of data should be higher.
# Sampling the data into 2/3 training and 1/3 testing
# Applying more complex algorithms like CNN, Deep Learning or others is not recommendable beacause it would make the model
# too complex and therefore not useful to make predictions on new data.
#### Bayer AG Data Science Interview ####
### Sentiment analysis
#install.packages("ggplot2")
#install.packages("magrittr")
#install.packages("dplyr")
#install.packages(c('e1071', 'rpart'))
#install.packages("tibble")
#install.packages("dplyr")
#install.packages("tm")
library(RTextTools)
library(dplyr)
library(e1071)
library(rpart)
#library(tm)
library(openxlsx)
library(ggplot2)
library(dplyr)
setwd("/Users/martinkurek/Documents/R Repo/bayer_case")
## Part 1: Vizualise distribution of sentiments
# Read in data
sm_data<- read.xlsx("data/sentences_with_sentiment.xlsx")
hist_data<- as.data.frame(sm_data[,3:5])
## Aggregate sum of columns
# Create the dataframe for historgram
Sentiment <- c("Positive","Negative", "Neutral")
Count <- c(sum(hist_data$Positive),sum(hist_data$Negative),sum(hist_data$Neutral))
total_count = sum(hist_data$Positive) + sum(hist_data$Negative) + sum(hist_data$Neutral)
Percent <- c(
paste(round((sum(hist_data$Positive) / total_count)*100), "%"),
paste(round((sum(hist_data$Negative) / total_count)*100), "%"),
paste(round((sum(hist_data$Neutral) / total_count)*100), "%")
)
hist_sum<- data.frame(Sentiment, Count, Percent)
#plot distribution with ggplot
g<- ggplot(hist_sum, aes(Sentiment, Count))
g + geom_col(fill = "blue") + geom_label(label= Percent)
## Result:
### Part 2: Sentiment Analysis
## Declare useful functions
#  listOfWords<- {c(
#    "pmlast"
#  )}
# Transform binary scema into nominal
assignSentiment <- function(x) {
i = 1
y <- data.frame()
for (i in i:nrow(hist_data)) {
if (hist_data[i, 1] == 1) {
y[i, 1] = "Positive"
} else if (hist_data[i, 2] == 1) {
y[i, 1] = "Negative"
} else {
y[i, 1] = "Neutral"
}
}
return(y)
}
# Generate Document Term Matrix with specified text preprocessing criteria
generateDTM <- function(z) {
y <- VCorpus(VectorSource(z[,1]))
y <- tm_map(y, content_transformer(tolower))
y <- tm_map(y, removeNumbers)
y <- tm_map(y, removePunctuation)
y <- tm_map(y, removeWords, stopwords("english"))
#y <- tm_map(y, removeWords, listOfWords)
y <- tm_map(y, stripWhitespace)
y <- DocumentTermMatrix(x = y)
return(y)
}
# Generate Document Term Matrix with same criteria
generateCorpus <- function(z) {
y <- VCorpus(VectorSource(z[,1]))
y <- tm_map(y, content_transformer(tolower))
y <- tm_map(y, removeNumbers)
y <- tm_map(y, removePunctuation)
y <- tm_map(y, removeWords, stopwords("english"))
#y <- tm_map(y, removeWords, listOfWords)
y <- tm_map(y, stripWhitespace)
return(y)
}
tdata <- as.data.frame(sm_data[,2])
sent <- assignSentiment(hist_data)
full_data <- cbind(tdata, sent)
# Treat rss_trn as tdata (Trainingdata) and rss_rel as sent
############ Text Mining ###########
dtm.train<- generateDTM(tdata)
corupus.train <- generateCorpus(tdata)
#Train & Testset
index <- 1:nrow(sent)
# Set Parameter for sampling
testindex <- sample(index, trunc(length(index)) / 1.3)
df.train <- sent[testindex, ]
df.test <- sent[-testindex, ]
trainset <- dtm.train[testindex, ]
testset <- dtm.train[-testindex, ]
index_data <- cbind(adtm.df, df.train)
corpus.ts <- corupus.train[testindex]
corpus.tss <- corupus.train[-testindex]
# Feature selection
dim(trainset)
# finding terms with lowest 2 and highest 6 frequency
ft <- findFreqTerms(trainset, 2, 5)
length((ft))
trainset <-
DocumentTermMatrix(corpus.ts, control = list(dictionary = ft))
testset <-
DocumentTermMatrix(corpus.tss, control = list(dictionary = ft))
#Train model with SVM
container <- create_container(trainset, df.train, trainSize = 1:nrow(trainset), virgin= FALSE)
pred.model <- train_model(container = container, algorithm = "SVM", kernel = "linear")
predicted <- predict(pred.model, newdata = testset)
table<- table("Predictions" = predicted, "Actual" = df.test)
print(table)
#Precision
nB_precision_ng<- round(table[1,1]/sum(table[,1]), 2)
nB_precision_nt<- round(table[2,2]/sum(table[,2]), 2)
nB_precision_p<- round(table[3,3]/sum(table[,3]), 2)
#Recall
nB_recall_ng<- round(table[1,1]/sum(table[1,]), 2)
nB_recall_nt<- round(table[2,2]/sum(table[2,]), 2)
nB_recall_p<- round(table[3,3]/sum(table[3,]), 2)
#Print Precision and Recall- Measures
print(c("Precision class 'Negative':", nB_precision_ng))
print(c("Recall class 'Negative':", nB_recall_ng))
print(c("Precision class 'Positive':",nB_precision_p))
print(c("Recall class 'Positive':",nB_recall_p))
print(c("Precision class 'Neutral':",nB_precision_nt))
print(c("Recall class 'Neutral':",nB_recall_nt))
#Print overall accuracy
print(paste("Accuracy:", (nB_precision_ng + nB_precision_nt + nB_precision_p + nB_recall_ng+ nB_recall_nt + nB_recall_p)/6))
## Result: The overall accuracy shows a maximum value to almost 0,6 which is insufficient.
# The precision- and recall values for the class 'Negative' are remarkeable low which is caused by the
# small percentage of negative comments in comparison with neutral and positve ones.
# In order to optimize the model the balance between all three classes should be more even as well as the amount of data should be higher.
# Sampling the data into 3/4 training and 1/4 testing with 942 frequent terms
# Applying more complex algorithms like CNN, Deep Learning or others is not recommendable beacause it would make the model
# too complex and therefore not useful to make predictions on new data.
library(tm)
ft
#### Bayer AG Data Science Interview ####
### Sentiment analysis
#install.packages("ggplot2")
#install.packages("magrittr")
#install.packages("dplyr")
#install.packages(c('e1071', 'rpart'))
#install.packages("tibble")
#install.packages("dplyr")
#install.packages("tm")
library(RTextTools)
library(dplyr)
library(e1071)
library(rpart)
library(tm)
library(openxlsx)
library(ggplot2)
library(dplyr)
setwd("/Users/martinkurek/Documents/R Repo/bayer_case")
## Part 1: Vizualise distribution of sentiments
# Read in data
sm_data <- read.xlsx("data/sentences_with_sentiment.xlsx")
hist_data <- as.data.frame(sm_data[, 3:5])
## Aggregate sum of columns
# Create the dataframe for historgram
Sentiment <- c("Positive", "Negative", "Neutral")
Count <-
c(sum(hist_data$Positive),
sum(hist_data$Negative),
sum(hist_data$Neutral))
total_count = sum(hist_data$Positive) + sum(hist_data$Negative) + sum(hist_data$Neutral)
Percent <- c(paste(round((
sum(hist_data$Positive) / total_count
) * 100), "%"),
paste(round((
sum(hist_data$Negative) / total_count
) * 100), "%"),
paste(round((
sum(hist_data$Neutral) / total_count
) * 100), "%"))
hist_sum <- data.frame(Sentiment, Count, Percent)
#plot distribution with ggplot
g <- ggplot(hist_sum, aes(Sentiment, Count))
g + geom_col(fill = "blue") + geom_label(label = Percent)
### Part 2: Sentiment Analysis
## Declare useful functions
# List of words function can be used for selection of features but was on purpose not applied due to lack of words.
#  listOfWords<- {c(
#    "pmlast"
#  )}
# Transform binary scema into nominal
assignSentiment <- function(x) {
i = 1
y <- data.frame()
for (i in i:nrow(hist_data)) {
if (hist_data[i, 1] == 1) {
y[i, 1] = "Positive"
} else if (hist_data[i, 2] == 1) {
y[i, 1] = "Negative"
} else {
y[i, 1] = "Neutral"
}
}
return(y)
}
# Generate Document Term Matrix with specified text preprocessing criteria
generateDTM <- function(z) {
y <- VCorpus(VectorSource(z[, 1]))
y <- tm_map(y, content_transformer(tolower))
y <- tm_map(y, removeNumbers)
y <- tm_map(y, removePunctuation)
y <- tm_map(y, removeWords, stopwords("english"))
#y <- tm_map(y, removeWords, listOfWords)
y <- tm_map(y, stripWhitespace)
y <- DocumentTermMatrix(x = y)
return(y)
}
# Generate Document Term Matrix with same criteria
generateCorpus <- function(z) {
y <- VCorpus(VectorSource(z[, 1]))
y <- tm_map(y, content_transformer(tolower))
y <- tm_map(y, removeNumbers)
y <- tm_map(y, removePunctuation)
y <- tm_map(y, removeWords, stopwords("english"))
#y <- tm_map(y, removeWords, listOfWords)
y <- tm_map(y, stripWhitespace)
return(y)
}
# Load data
tdata <- as.data.frame(sm_data[, 2])
sent <- assignSentiment(hist_data)
full_data <- cbind(tdata, sent)
# Treat rss_trn as tdata (Trainingdata) and rss_rel as sent
############ Text Mining ###########
dtm.train <- generateDTM(tdata)
corupus.train <- generateCorpus(tdata)
#Train & Testset
index <- 1:nrow(sent)
# Set Parameter for sampling (75% Training, 25% Testing) by defining and applying index
testindex <- sample(index, trunc(length(index)) / 1.333)
df.train <- sent[testindex,]
df.test <- sent[-testindex,]
trainset <- dtm.train[testindex,]
testset <- dtm.train[-testindex,]
index_data <- cbind(adtm.df, df.train)
corpus.ts <- corupus.train[testindex]
corpus.tss <- corupus.train[-testindex]
# Feature selection by chosing frequency interval
dim(trainset)
# finding terms with lowest 2 and highest 5 frequency
ft <- findFreqTerms(trainset, 2, 5)
length((ft))
trainset <-
DocumentTermMatrix(corpus.ts, control = list(dictionary = ft))
testset <-
DocumentTermMatrix(corpus.tss, control = list(dictionary = ft))
#Train model with SVM
container <-
create_container(trainset,
df.train,
trainSize = 1:nrow(trainset),
virgin = FALSE)
pred.model <-
train_model(container = container,
algorithm = "SVM",
kernel = "radial")
predicted <- predict(pred.model, newdata = testset)
table <- table("Predictions" = predicted, "Actual" = df.test)
#Print Confusion Matrix
print(table)
#Precision
nB_precision_ng <- round(table[1, 1] / sum(table[, 1]), 2)
nB_precision_nt <- round(table[2, 2] / sum(table[, 2]), 2)
nB_precision_p <- round(table[3, 3] / sum(table[, 3]), 2)
#Recall
nB_recall_ng <- round(table[1, 1] / sum(table[1, ]), 2)
nB_recall_nt <- round(table[2, 2] / sum(table[2, ]), 2)
nB_recall_p <- round(table[3, 3] / sum(table[3, ]), 2)
#Print Precision and Recall- Measures
print(c("Precision class 'Negative':", nB_precision_ng))
print(c("Recall class 'Negative':", nB_recall_ng))
print(c("Precision class 'Positive':", nB_precision_p))
print(c("Recall class 'Positive':", nB_recall_p))
print(c("Precision class 'Neutral':", nB_precision_nt))
print(c("Recall class 'Neutral':", nB_recall_nt))
#Print overall accuracy
print(paste(
"Accuracy:",
(
nB_precision_ng + nB_precision_nt + nB_precision_p + nB_recall_ng + nB_recall_nt + nB_recall_p
) / 6
))
## Result:
# The overall accuracy shows a maximum value to almost 0,6 which is insufficient.
# The precision- and recall values for the class 'Negative' are remarkeable low which is caused by
# small occurencies of negative comments in comparison with neutral and positve ones.
# In order to optimize the model, the balance between all three classes should be more even as well as the amount of data higher.
# Sampling the data into 3/4 training and 1/4 testing with 356 frequent terms out of 1178 in total shows that the data in not able to provide enough
# relevant sentences to build a sustainable model.
# It also has to be mentioned, that deleting outliers out of the dataset would also lead to an higher accuracy of the model.
# This approach has not been proceed due to time issues. Nevertheless the steps in general would be recommended as the following:
# a) Finding the optimum number of clusters (k), b) Applying k-means clustering with determined k, c) Label clustered data with clusters
# and filter out cluster(s) that could consist of outliers.
# Applying more complex algorithms like CNN, Deep Learning or others in order to train the model is not recommendable
# beacause it would make the model too complex and therefore not useful to make predictions on new data.
#### Bayer AG Data Science Interview ####
### Sentiment analysis
#install.packages("ggplot2")
#install.packages("magrittr")
#install.packages("dplyr")
#install.packages(c('e1071', 'rpart'))
#install.packages("tibble")
#install.packages("dplyr")
#install.packages("tm")
library(RTextTools)
library(dplyr)
library(e1071)
library(rpart)
library(tm)
library(openxlsx)
library(ggplot2)
library(dplyr)
setwd("/Users/martinkurek/Documents/R Repo/bayer_case")
## Part 1: Vizualise distribution of sentiments
# Read in data
sm_data <- read.xlsx("data/sentences_with_sentiment.xlsx")
hist_data <- as.data.frame(sm_data[, 3:5])
## Create the dataframe for historgram
Sentiment <- c("Positive", "Negative", "Neutral")
Count <-
c(sum(hist_data$Positive),
sum(hist_data$Negative),
sum(hist_data$Neutral))
total_count = sum(hist_data$Positive) + sum(hist_data$Negative) + sum(hist_data$Neutral)
Percent <- c(paste(round((
sum(hist_data$Positive) / total_count
) * 100), "%"),
paste(round((
sum(hist_data$Negative) / total_count
) * 100), "%"),
paste(round((
sum(hist_data$Neutral) / total_count
) * 100), "%"))
hist_sum <- data.frame(Sentiment, Count, Percent)
#Plot distribution with ggplot
g <- ggplot(hist_sum, aes(Sentiment, Count))
g + geom_col(fill = "blue") + geom_label(label = Percent)
### Part 2: Sentiment Analysis
## Declare useful functions
# List of words function can be used for selection of features but was on purpose not applied due to lack of words.
#  listOfWords<- {c(
#    "pmlast"
#  )}
# Transform binary scema into nominal
assignSentiment <- function(x) {
i = 1
y <- data.frame()
for (i in i:nrow(hist_data)) {
if (hist_data[i, 1] == 1) {
y[i, 1] = "Positive"
} else if (hist_data[i, 2] == 1) {
y[i, 1] = "Negative"
} else {
y[i, 1] = "Neutral"
}
}
return(y)
}
# Generate Document Term Matrix with specified text preprocessing criteria
generateDTM <- function(z) {
y <- VCorpus(VectorSource(z[, 1]))
y <- tm_map(y, content_transformer(tolower))
y <- tm_map(y, removeNumbers)
y <- tm_map(y, removePunctuation)
y <- tm_map(y, removeWords, stopwords("english"))
#y <- tm_map(y, removeWords, listOfWords)
y <- tm_map(y, stripWhitespace)
y <- DocumentTermMatrix(x = y)
return(y)
}
# Generate Document Term Matrix with same criteria
generateCorpus <- function(z) {
y <- VCorpus(VectorSource(z[, 1]))
y <- tm_map(y, content_transformer(tolower))
y <- tm_map(y, removeNumbers)
y <- tm_map(y, removePunctuation)
y <- tm_map(y, removeWords, stopwords("english"))
#y <- tm_map(y, removeWords, listOfWords)
y <- tm_map(y, stripWhitespace)
return(y)
}
# Load data
tdata <- as.data.frame(sm_data[, 2])
sent <- assignSentiment(hist_data)
full_data <- cbind(tdata, sent)
# Treat rss_trn as tdata (Trainingdata) and rss_rel as sent
############ Text Mining ###########
dtm.train <- generateDTM(tdata)
corupus.train <- generateCorpus(tdata)
##Split into Train & Testset
index <- 1:nrow(sent)
# Set Parameter for sampling (75% Training, 25% Testing) by defining and applying index
testindex <- sample(index, trunc(length(index)) / 1.333)
df.train <- sent[testindex,]
df.test <- sent[-testindex,]
trainset <- dtm.train[testindex,]
testset <- dtm.train[-testindex,]
index_data <- cbind(adtm.df, df.train)
corpus.ts <- corupus.train[testindex]
corpus.tss <- corupus.train[-testindex]
## Feature selection by chosing frequency interval
dim(trainset)
# finding terms with lowest 2 and highest 5 frequency
ft <- findFreqTerms(trainset, 2, 5)
length((ft))
trainset <-
DocumentTermMatrix(corpus.ts, control = list(dictionary = ft))
testset <-
DocumentTermMatrix(corpus.tss, control = list(dictionary = ft))
##Train model with SVM
container <-
create_container(trainset,
df.train,
trainSize = 1:nrow(trainset),
virgin = FALSE)
##Linear Kernel because values are either 0 or 1 and evaluation of applying both showed linear fits better
pred.model <-
train_model(container = container,
algorithm = "SVM",
kernel = "linear")
predicted <- predict(pred.model, newdata = testset)
## Evaluate Model
table <- table("Predictions" = predicted, "Actual" = df.test)
#Print Confusion Matrix
print(table)
#Precision
nB_precision_ng <- round(table[1, 1] / sum(table[, 1]), 2)
nB_precision_nt <- round(table[2, 2] / sum(table[, 2]), 2)
nB_precision_p <- round(table[3, 3] / sum(table[, 3]), 2)
#Recall
nB_recall_ng <- round(table[1, 1] / sum(table[1, ]), 2)
nB_recall_nt <- round(table[2, 2] / sum(table[2, ]), 2)
nB_recall_p <- round(table[3, 3] / sum(table[3, ]), 2)
#Print Precision and Recall- Measures
print(c("Precision class 'Negative':", nB_precision_ng))
print(c("Recall class 'Negative':", nB_recall_ng))
print(c("Precision class 'Positive':", nB_precision_p))
print(c("Recall class 'Positive':", nB_recall_p))
print(c("Precision class 'Neutral':", nB_precision_nt))
print(c("Recall class 'Neutral':", nB_recall_nt))
#Print overall accuracy
print(paste(
"Accuracy:",
(
nB_precision_ng + nB_precision_nt + nB_precision_p + nB_recall_ng + nB_recall_nt + nB_recall_p
) / 6
))
### Part3: Result
# The overall accuracy shows a maximum value to almost 0,6 which is insufficient.
# The precision- and recall values for the class 'Negative' are remarkeable low which is caused by
# small occurencies of negative comments in comparison with neutral and positve ones.
# In order to optimize the model, the balance between all three classes should be more even as well as the amount of data higher.
# Sampling the data into 3/4 training and 1/4 testing with 356 frequent terms out of 1178 in total shows that the data in not able to provide enough
# relevant sentences to build a sustainable model.
# It also has to be mentioned, that deleting outliers out of the dataset would also lead to an higher accuracy of the model.
# This approach has not been proceed due to time issues. Nevertheless the steps in general would be recommended as the following:
# a) Finding the optimum number of clusters (k), b) Applying k-means clustering with determined k, c) Label clustered data with clusters
# and filter out cluster(s) that could consist of outliers.
# Applying more complex algorithms like CNN, Deep Learning or others in order to train the model is not recommendable
# beacause it would make the model too complex and therefore not useful to make predictions on new data.
ft
